---
layout: page
title: Jobs
permalink: /jobs/
---

<img src="../images/Jobs.png">
<div class="fun-caption">
<br />
</div>

<h2>Job Openings</h2>
<ul>
<li><a href="#opthamalogy_postdoc">Postdoctoral Researcher - Machine Learning and Opthamology</a>, <a href="../job_postings/Retina_PostDoc_2019_QTIM.pdf">PDF</a></li>
</ul>
<hr />
<h3 id="opthamalogy_postdoc">Introduction</h3>
We are QTIM (https://qtim-lab.github.io)â€” the Quantitative Translational Imaging in Medicine Lab at the Athinoula A. Martinos Center of the Massachusetts General Hospital (MGH). We are affiliated with the Health Sciences and Technology Program at MIT and Harvard Medical School. We are computer scientists, medical physicists, neuro-oncologists, MRI technicians, and clinical research coordinators. We focus broadly on quantitative and machine learning techniques in medical imaging, with a growing interest in deep learning. We try to bridge the gap between machine learning research and clinical practice through fruitful connections with our clinical collaborators in and out of the MGH ecosystem.
<br />
<h3>Postdoctoral Researcher - Machine Learning and Opthamology</h3>
You will be a postdoctoral research fellow working in the exciting and dynamic fields of deep learning and medical imaging. Your main focus will be the development of novel deep learning algorithms for automated diagnosis, monitoring and risk assessment in retinopathy of prematurity (ROP), and other ophthalmic diseases. You will collaborate with clinician scientists at Oregon Health & Science University and other affiliated institutions to build solutions to real-world problems. 
<br /><br />
Outside of your own projects, you will also have responsibility for mentoring graduate students working on related projects. You will also have the opportunity to pursue theoretical projects on novel datasets, and collaborate with clinicians at MGH from a wide range of fields. You will be encouraged and supported to seek independent funding as you take the next step in your academic career.
<br />
<h3 />Essential criteria</h3>
PhD in computer science or related discipline such as mathematics, engineering or physics.
<ul>
<li>Experience in image processing or computer vision techniques</li>
<li>Proficiency in Python, C++, or other languages such as MATLAB and R</li>
<li>Comfortable using Linux environments and the terminal for running jobs non-interactively</li>
</ul>
<h3>Desirable skills</h3>
We will consider applicants with any or all of the skills described below, or an ability to learn them independently.
<ul>
<li>Familiarity with machine learning and deep learning packages such as PyTorch and TensorFlow.</li>
<li>Experience in working with medical images, particularly fundus photography and optical coherence tomography</li>
<li>Experience in using containerization software such as Docker</li>
</ul>
<h3>Why work with us?</h3>
We believe that our research has real potential to improve the standard of care for patients worldwide. If you join us, you will become an integral part of an interdisciplinary team that values collaboration and teamwork above all else. We have a history of publications in high impact journals, and you will be encouraged and supported to contribute to that effort. Further, we believe that a healthy work-life balance is the key to a success academic life, and regularly take time out to go bowling, climbing, and cooking!
<br />
<h3>How to apply</h3>
Do you want find out more? We would love to hear from you. Please send a message to kalpathy@nmr.mgh.harvard.edu with your C.V. and the contact information for three references. 
<br /><br />
We are an equal opportunity employer and value diversity. We also know that the work of diversity and anti-discrimination extends past choices in hiring. We work every single day to make our lab an equitable and productive space for everyone, regardless of their race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.
<br /><br />
<hr>
<br /><br />

If none of the job opportunities above fit your goals, feel free to get in touch at <a href="mailto:qtim.lab@gmail.com">qtim.lab@gmail.com</a> if you are still interested in working with us in any capacity.
<!-- Regardless, feel free to get in touch at <a href="mailto:qtim.lab@gmail.com">qtim.lab@gmail.com</a> if you are interested in working with us in any capacity. -->